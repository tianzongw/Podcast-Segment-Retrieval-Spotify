{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the data looks like \n",
    "\n",
    "for each episode there is a json-file with following structure\n",
    "\n",
    "{\"results\":[{\"alternatives\":[{\"transcript\":\"Hi, this is transcript of first segment\",\"confidence\":0.91,\"words\":[{\n",
    "                     \"startTime\":\"0s\",\n",
    "                     \"endTime\":\"0.300s\",\n",
    "                     \"word\":\"Hi,\"\n",
    "                  },\n",
    "                  {\"startTime\":\"0.500s\",\n",
    "                     \"endTime\":\"0.900s\",\n",
    "                     \"word\":\"this\"}]}]}]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What I want the data to look like (kind of)\n",
    "\n",
    "for each show there shoud be one dict containing show id, show name and all episodes of show split into segments\n",
    "\n",
    "[{'show-id':'123','show-name':'alex & sigge','episodes':[{'name':'first episode','date':'2020-01-01','segmentTranscripts':[{'segNo':0,'startTime':'00:00s','endTime':'00:20s','transcript':'......',},{},]}]},{},]\n",
    "\n",
    "- show name would have to be looked up in the metadata.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#for each episode extract {word:'hello',timestamp:'00.00s'} \n",
    "    \n",
    "\n",
    "def extract_segments(path):\n",
    "    with open(path, \"r\") as read_file:\n",
    "        episode = json.load(read_file)\n",
    "    segments=[]\n",
    "    #had to do \"manual\" iteration due to irregularities in data\n",
    "    iter=0\n",
    "    for segment in episode[\"results\"]:\n",
    "        seg_result={}\n",
    "        #make sure there is only one dict in this list (should be true according to dataset description)\n",
    "        assert len(segment[\"alternatives\"])==1\n",
    "        segment_dict=segment[\"alternatives\"][0]\n",
    "        #sometimes \"alternatives\" dict is empty...\n",
    "        if \"words\" and \"transcript\"  in segment_dict:\n",
    "            #add segment number\n",
    "            seg_result[\"segNum\"]=iter\n",
    "            #add timestamp of the first word in this segment\n",
    "            seg_result[\"startTime\"]=segment_dict[\"words\"][0][\"startTime\"]\n",
    "            #add timestamp of the last word in this segment\n",
    "            seg_result[\"endTime\"]=segment_dict[\"words\"][-1][\"endTime\"]\n",
    "            #add transcript of this segment \n",
    "            seg_result[\"transcript\"]=segment_dict[\"transcript\"]\n",
    "            segments.append(seg_result)\n",
    "            iter+=1\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['3GsS8PAYL6u3ClsZFnW3RZ'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_segments('/Users/Simpan/spotify/spotify-podcasts-2020/podcasts-transcripts/7/0/show_70AtBgIej68YuFXDt6l0aB/3GsS8PAYL6u3ClsZFnW3RZ.json').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tianzong/Desktop/spotify/Podcast-Segment-Retrieval-Spotify/src\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from training_data_collection import collect_training_episodes\n",
    "\n",
    "#TODO: formalize data organization, no hard codes\n",
    "input_file = '../data/podcasts_2020_train.1-8.qrels.txt'\n",
    "root_dir = '../data/podcasts-no-audio-13GB'\n",
    "training_episodes = collect_training_episodes(root_dir, input_file)\n",
    "training_segments = {}\n",
    "for episode in training_episodes:\n",
    "    episode_id=episode.split('/')[-1].split('.json')[0]\n",
    "    training_segments[episode_id]=extract_segments(episode)\n",
    "with open('../data/training_sub.json', 'w') as fout:\n",
    "        json.dump(training_segments , fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "path='../data/training_sub.json'\n",
    "with open(path,'r') as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch queries from xml and put into list\n",
    "#def fetch_queries(path)\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('../../podcasts_2020_topics_train.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "topics=[{'topic-no':element[0].text,'query':element[2].text,'description':element[3].text} for element in root]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch topic number, correct jump-in point for training set\n",
    "\n",
    "with open('../../podcasts_2020_train.1-8.qrels.txt','r') as f:\n",
    "    contents=f.readlines()\n",
    "targets=[{'episode':line.split()[2].split('_')[0].split(':')[2],'topic-no':line[0],'target-time':line.split()[2].split('_')[1]} for line in contents]\n",
    "\n",
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO \n",
    "#match episodes, target times and queries\n",
    "#rewrite semi pseudo-code\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "\n",
    "#semi pseduo-code\n",
    "n_correct=0\n",
    "for episode in data:\n",
    "    episode_id=\n",
    "    target_time=\n",
    "    data=extract_segments(path)\n",
    "    segments=[item[\"transcript\"] for item in data]\n",
    "    timespans=[(item[\"startTime\"],item[\"stopTime\"]) for item in data]\n",
    "    query=topics[episode['episode-no']]['query']\n",
    "    episode_embeddings=embedder.encode(segments, convert_to_tensor=True)\n",
    "    query_embedding=embedder(query)\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    #if running on gpu\n",
    "    #if cuda.device()==GPU\n",
    "    #cos_scores = cos_scores.cpu()\n",
    "    top_k=5\n",
    "    top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "    idx_closest=top_results[0]\n",
    "    pred_timespan=timespans[idx_closest]\n",
    "    if pred_timespan[0]<=target_timespan<=pred_timespan[1]:\n",
    "        n_correct+=1\n",
    "\n",
    "\n",
    "accuracy=n_correct/len(test_dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dont run\n",
    "import os\n",
    "\n",
    "base_dir=\"/Users/Simpan/spotify/spotify-podcasts-2020/podcasts-transcripts\"\n",
    "data=[]\n",
    "for sub_dir in os.listdir(base_dir):\n",
    "    for sub_sub_dir in os.listdir(base_dir+'/'+sub_dir):\n",
    "        for show_dir in os.listdir(base_dir+'/'+sub_dir+'/'+sub_sub_dir):\n",
    "            show_dict={}\n",
    "            show_dict[\"showId\"]=show_dir\n",
    "            show_dict[\"episodes\"]=[]\n",
    "            for i,episode in enumerate(os.listdir(base_dir+'/'+sub_dir+'/'+sub_sub_dir+'/'+show_dir)):\n",
    "                episode_dict={}\n",
    "                #remove .json ending\n",
    "                episode_dict[\"episodeId\"]=episode[:-5]\n",
    "                episode_dict[\"episodeNo\"]=i\n",
    "                episode_path=base_dir+'/'+sub_dir+'/'+sub_sub_dir+'/'+show_dir+'/'+episode\n",
    "                episode_dict[\"episodeSegments\"]=extract_segments(episode_path)\n",
    "                show_dict[\"episodes\"].append(episode_dict)\n",
    "            data.append(show_dict)\n",
    "with open(\"test.json\", \"w+\") as write_file:\n",
    "    json.dump(data, write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
