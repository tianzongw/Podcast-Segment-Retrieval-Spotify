{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mimig/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import rank_bm25 as bm25\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = '/media/sf_year2/DD2430-Project/Podcast-Segment-Retrieval-Spotify/'\n",
    "os.chdir(wd)\n",
    "files = os.listdir('./data/training_episodes')\n",
    "files = [f for f in files if f != 'place_holder']\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mimig/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils_stats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = \"./data/metadata.tsv\"\n",
    "metadata = pd.read_csv(metadata_file, sep=\"\\t\")\n",
    "metadata['show_id'] = metadata.apply(lambda x: x['show_uri'].split(\":\")[2], axis=1)\n",
    "metadata['episode_id'] = metadata.apply(lambda x: x['episode_uri'].split(\":\")[2], axis=1)\n",
    "metadata.set_index('episode_id', inplace=True)\n",
    "# metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_file = \"./data/podcasts_2020_train.1-8.qrels.txt\"\n",
    "qrels = pd.read_csv(qrels_file, sep=\"\\t\", header=None)#\n",
    "qrels.set_axis(['topic', '1', 'episode_start', 'score'], axis=1, inplace=True)\n",
    "qrels['episode'] = qrels.apply(lambda x: x['episode_start'].split(\":\")[2].split(\"_\")[0], axis=1)\n",
    "qrels['start'] = qrels.apply(lambda x: int(x['episode_start'].split(\"_\")[1].split(\".\")[0]), axis=1)\n",
    "qrels['end'] = qrels.apply(lambda x: int(x['start'] + 120), axis=1)\n",
    "qrels['interval'] = qrels.apply(lambda x: (x['start'], x['end']), axis=1)\n",
    "# qrels = qrels[qrels.score>0]\n",
    "# qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/training_sub_episode_loc.out\", \"r\") as f:\n",
    "    files = f.readlines()\n",
    "    \n",
    "files = [f.replace(\"\\n\", \"\") for f in files]\n",
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overlapping(x, y):\n",
    "    return x[0] <= y[1] and y[0] <= x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_sub = {}\n",
    "\n",
    "# for filename in files: \n",
    "#     training_sub[os.path.splitext(filename)[0].split('/')[-1]] = extract_segments(filename)\n",
    "\n",
    "# with open(\"./data/training_sub_cleaned.json\", \"w\") as f:\n",
    "#     json.dump(training_sub, f)\n",
    "\n",
    "with open(\"./data/training_sub_cleaned.json\", \"r\") as f:\n",
    "    training_sub = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training_sub\n",
    "tf_idf, dct, corpus, ix2episode, full_texts = get_tf_idf_gensim(training_sub)\n",
    "# ix2episode = list(training.keys())\n",
    "# full_texts_clean = [tokenize(training[k]['full_text_clean']) for k in ix2episode]\n",
    "# full_texts = [training[k]['full_text']for k in ix2episode]\n",
    "\n",
    "# dct = Dictionary(full_texts_clean)  # fit dictionary\n",
    "# corpus = [dct.doc2bow(line) for line in full_texts_clean] \n",
    "# model = TfidfModel(corpus) \n",
    "# model[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf, ix2episode, full_texts = get_tf_idf(training_sub, n=1)\n",
    "topics = extract_topics(\"./data/podcasts_2020_topics_train.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 8 facebook stock prediction: After Facebook’s Q4 2018 earnings call, what were experts’ predictions and expectations for its stock price in 2019?  How did these predictions fare over time?  Relevant material would include predictions immediately following the Jan 29, 2019, earnings call, and later actual stock performance that could be used to evaluate predictions.\n"
     ]
    }
   ],
   "source": [
    "# for ti, topic_dict in topics.items():\n",
    "#     print(\"Topic {} {}: {}\".format(ti, topic_dict['query'], topic_dict['description']))\n",
    "# #     print(\"The topic is: {}\".format(topic))\n",
    "#     top_episodes = get_topic_episodes(topics, ti, tf_idf, ix2episode)\n",
    "#     for ei, e in enumerate(top_episodes):\n",
    "        \n",
    "#         print(\"Result {}: \\n{}: {}\".format(ei, metadata.episode_name[e], metadata.episode_description[e]))\n",
    "#     print(\"\\n\")\n",
    "ti = 8\n",
    "topic_dict = topics[ti]\n",
    "print(\"Topic {} {}: {}\".format(ti, topic_dict['query'], topic_dict['description']))\n",
    "#     print(\"The topic is: {}\".format(topic))\n",
    "# top_episodes = get_topic_episodes(topics, ti, tf_idf, ix2episode, n_episodes=60)\n",
    "top_episodes = get_topic_episodes_gensim(topics, ti, tf_idf, dct, corpus, ix2episode, n_episodes=60)\n",
    "# for ei, e in enumerate(top_episodes):\n",
    "\n",
    "#     print(\"Result {}: \\n{}: {}\".format(ei, metadata.episode_name[e], metadata.episode_description[e]))\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=2\n",
    "# n_episodes=10\n",
    "# tokenized_query = tokenize(topic_dict['query'], n)\n",
    "# tokenized_query.extend(tokenize(clean_text(topic_dict['description']), n))\n",
    "# # sum([x[1] for x in model[dct.doc2bow(tokenized_query)]])\n",
    "# N = len(corpus)\n",
    "# tokens = dct.doc2idx(tokenized_query)\n",
    "# score = np.zeros((N))\n",
    "# for doc in range(N):\n",
    "#     doc_scores = [x[1] for x in model[corpus[doc]] if x[0] in tokens]\n",
    "#     score[doc] = sum(doc_scores)\n",
    "\n",
    "# ixs = np.argsort(-score)[0:n_episodes]\n",
    "# top_episodes = [ix2episode[i] for i in ixs]\n",
    "# top_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "# flat_segments = flatten(all_segments)\n",
    "# tokenized_segments = [x.split(\" \") for x in flat_segments]\n",
    "# tokenized_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5447 5447 5447 5447 5447\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "i2episode = [t for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "i2segnum = [sd['segNum'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "i2transcript = [sd['transcript'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "i2transcript_clean = [sd['transcript_clean'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "i2tokens = [tokenize(x, n) for x in i2transcript_clean]\n",
    "ix = list(range(len(i2segnum)))\n",
    "\n",
    "print(len(i2episode), len(i2segnum), len(i2transcript), len(i2transcript_clean), len(i2tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25 = BM25Okapi(tokenized_segments)\n",
    "\n",
    "# query = \"climate change strike\"\n",
    "# tokenized_query = query.split(\" \")\n",
    "\n",
    "# bm25.get_top_n(tokenized_query, flat_segments, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46.980417593933474,\n",
       " 43.25303742716171,\n",
       " 39.36840986485864,\n",
       " 38.767809099050815,\n",
       " 38.562615046608805,\n",
       " 37.80248140741179,\n",
       " 36.11133454563174,\n",
       " 35.89004635633567,\n",
       " 35.72021091560965,\n",
       " 35.6141814888667]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = bm25.BM25Okapi(i2tokens)\n",
    "\n",
    "tokenized_query = tokenize(topic_dict['query'], n)\n",
    "tokenized_query.extend(tokenize(clean_text(topic_dict['description']), n))\n",
    "choices = model.get_top_n(tokenized_query, ix, 10)\n",
    "scores = model.get_scores(tokenized_query)\n",
    "[scores[i] for i in choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_details(choices, i2segnum, i2episode, i2transcript):\n",
    "    result = {}\n",
    "    for ix, i in enumerate(choices):\n",
    "        result[\"spotify:episode:{}_{}.0\".format(i2episode[i], 30*i2segnum[i])] = {\n",
    "            \"episode\": i2episode[i],\n",
    "            \"start\": 30*i2segnum[i],\n",
    "            \"end\": 30*i2segnum[i] + 120,\n",
    "            \"interval\": (30*i2segnum[i], 30*i2segnum[i] + 120),\n",
    "            \"transcript\": i2transcript[i],\n",
    "            \"rank\": ix\n",
    "        }\n",
    "        \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = get_segment_details(choices, i2segnum, i2episode, i2transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spotify:episode:5UGPiylWqD8IXIRflWXJEg_1620.0',\n",
       " 'spotify:episode:5UGPiylWqD8IXIRflWXJEg_1650.0',\n",
       " 'spotify:episode:0rBkvc7wrAf5sLXbMuk0QO_2760.0',\n",
       " 'spotify:episode:55VfynKK09foFInjoBjldC_150.0',\n",
       " 'spotify:episode:5UGPiylWqD8IXIRflWXJEg_780.0',\n",
       " 'spotify:episode:5EJg9dgAMD1HhlRSN99ttX_1350.0',\n",
       " 'spotify:episode:07XpAEJ0l0Oo40zFdswi8m_300.0',\n",
       " 'spotify:episode:7E2zOJ4ADY2NkgODLwO7fe_3240.0',\n",
       " 'spotify:episode:1xKhK3czSfBiKgnSPfzskv_600.0',\n",
       " 'spotify:episode:0rBkvc7wrAf5sLXbMuk0QO_2730.0']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(top10.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'query': 'coronavirus spread',\n",
       "  'type': 'topical',\n",
       "  'description': 'What were people saying about the spread of the novel coronavirus NCOV-19 in Wuhan at the end of 2019?',\n",
       "  'description_clean': 'people saying spread novel coronavirus ncov wuhan end '},\n",
       " 2: {'query': 'greta thunberg cross atlantic',\n",
       "  'type': 'topical',\n",
       "  'description': 'What were people saying about Greta Thunberg’s sailing trip across the Atlantic Ocean in the fall of 2019 and its relationship to global climate change?',\n",
       "  'description_clean': 'people saying greta thunberg’s sailing trip atlantic ocean fall  relationship global climate change'},\n",
       " 3: {'query': 'black hole image',\n",
       "  'type': 'topical',\n",
       "  'description': 'In May 2019 astronomers released the first-ever picture of a black hole.  I would like to hear some conversations and educational discussion about the science of astronomy, black holes, and of the picture itself.',\n",
       "  'description_clean': ' astronomer released firstever picture black hole like hear conversation educational discussion science astronomy black hole picture'},\n",
       " 4: {'query': 'story about riding a bird',\n",
       "  'type': 'refinding',\n",
       "  'description': 'I remember hearing a podcast that had a story about a kid riding some kind of bird.  I want to find it again.',\n",
       "  'description_clean': 'remember hearing podcast story kid riding kind bird want'},\n",
       " 5: {'query': 'daniel ek interview',\n",
       "  'type': 'known item',\n",
       "  'description': 'Someone told me about a podcast interview with Daniel Ek, CEO of Spotify, about the founding and early days of Spotify.  I would like to find the show and episode that contains that interview.  Other interviews with Ek are relevant as well.',\n",
       "  'description_clean': 'told podcast interview daniel ek ceo spotify founding early day spotify like episode contains interview interview ek relevant'},\n",
       " 6: {'query': 'michelle obama becoming',\n",
       "  'type': 'topical',\n",
       "  'description': 'Former First Lady Michelle Obama’s memoir Becoming was published in early 2019.  What were people saying about it?',\n",
       "  'description_clean': 'lady michelle obama’s memoir published early  people saying'},\n",
       " 7: {'query': 'anna delvey',\n",
       "  'type': 'topical',\n",
       "  'description': 'Anna Sorokina moved to New York City in 2013 and posed as wealthy German heiress Anna Delvey.  In 2019 she was convicted of grand larceny, theft, and fraud.  What were people saying about her, the charges, her trial, and New York socialite society in general?',\n",
       "  'description_clean': 'anna sorokina moved new york city  posed wealthy german heiress anna delvey  convicted grand larceny theft fraud people saying charge trial new york socialite society general'},\n",
       " 8: {'query': 'facebook stock prediction',\n",
       "  'type': 'topical',\n",
       "  'description': 'After Facebook’s Q4 2018 earnings call, what were experts’ predictions and expectations for its stock price in 2019?  How did these predictions fare over time?  Relevant material would include predictions immediately following the Jan 29, 2019, earnings call, and later actual stock performance that could be used to evaluate predictions.',\n",
       "  'description_clean': 'facebook’s q  earnings experts’ prediction expectation stock price  prediction fare time relevant material include prediction immediately following jan   earnings later actual stock performance evaluate prediction'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_qrels(results, qrels, topic):\n",
    "    df = qrels[qrels.topic == topic]\n",
    "    for k, v in results.items():\n",
    "        e = v['episode']\n",
    "        i = v['interval']\n",
    "#         print(np.sum(df.episode==e))\n",
    "        if np.sum(df.episode==e)>0:\n",
    "            ixs = df.episode == e\n",
    "            for episode, interval, score in zip(df.episode_start[ixs], df.interval[ixs], df.score[ixs]):\n",
    "                if is_overlapping(i, interval):\n",
    "                    print(\"{} is overlapping with choice in qrels with score {}\".format(k, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotify:episode:5UGPiylWqD8IXIRflWXJEg_1620.0 is overlapping with choice in qrels with score 0\n",
      "spotify:episode:5UGPiylWqD8IXIRflWXJEg_1650.0 is overlapping with choice in qrels with score 0\n",
      "spotify:episode:55VfynKK09foFInjoBjldC_150.0 is overlapping with choice in qrels with score 3\n",
      "spotify:episode:55VfynKK09foFInjoBjldC_150.0 is overlapping with choice in qrels with score 1\n",
      "spotify:episode:5UGPiylWqD8IXIRflWXJEg_780.0 is overlapping with choice in qrels with score 0\n",
      "spotify:episode:07XpAEJ0l0Oo40zFdswi8m_300.0 is overlapping with choice in qrels with score 0\n",
      "spotify:episode:7E2zOJ4ADY2NkgODLwO7fe_3240.0 is overlapping with choice in qrels with score 0\n"
     ]
    }
   ],
   "source": [
    "compare_with_qrels(top10, qrels, ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 coronavirus spread: What were people saying about the spread of the novel coronavirus NCOV-19 in Wuhan at the end of 2019?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfModel' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fde806336f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Topic {} {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtop_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix2episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mi2episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_episodes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mi2segnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segNum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_episodes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sf_year2/DD2430-Project/Podcast-Segment-Retrieval-Spotify/src/utils_stats.py\u001b[0m in \u001b[0;36mget_topic_episodes\u001b[0;34m(topics, ix, tf_idf, ix2episode, n_episodes)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfModel' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for ti, topic_dict in topics.items():\n",
    "\n",
    "    print(\"Topic {} {}: {}\".format(ti, topic_dict['query'], topic_dict['description']))\n",
    "    top_episodes = get_topic_episodes(topics, ti, tf_idf, ix2episode)\n",
    "    i2episode = [t for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2segnum = [sd['segNum'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2transcript = [sd['transcript'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2transcript_clean = [sd['transcript_clean'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2tokens = [tokenize(x) for x in i2transcript_clean]\n",
    "    ix = list(range(len(i2segnum)))\n",
    "    \n",
    "    model = bm25.BM25Okapi(i2tokens)\n",
    "\n",
    "    tokenized_query = tokenize(topic_dict['query'])\n",
    "    tokenized_query.extend(tokenize(clean_text(topic_dict['description'])))\n",
    "    choices = model.get_top_n(tokenized_query, ix, 10)\n",
    "    \n",
    "    top10 = get_segment_details(choices, i2segnum, i2episode, i2transcript)\n",
    "    \n",
    "    compare_with_qrels(top10, qrels, ti)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "for ti, topic_dict in topics.items():\n",
    "\n",
    "    print(\"Topic {} {}: {}\".format(ti, topic_dict['query'], topic_dict['description']))\n",
    "    top_episodes = get_topic_episodes(topics, ti, tf_idf, ix2episode)\n",
    "    i2episode = [t for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2segnum = [sd['segNum'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2transcript = [sd['transcript'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2transcript_clean = [sd['transcript_clean'] for t in top_episodes for sd in training_sub[t]['segments']]\n",
    "    i2tokens = [tokenize(x, n) for x in i2transcript_clean]\n",
    "    ix = list(range(len(i2segnum)))\n",
    "    \n",
    "    model = bm25.BM25Okapi(i2tokens)\n",
    "\n",
    "    tokenized_query = tokenize(topic_dict['query'], n)\n",
    "    tokenized_query.extend(tokenize(clean_text(topic_dict['description']), n))\n",
    "    choices = model.get_top_n(tokenized_query, ix, 10)\n",
    "    all_scores = model.get_scores(tokenized_query)\n",
    "    scores = [all_scores[i] for i in choices]\n",
    "    top10 = get_segment_details(choices, i2segnum, i2episode, i2transcript)\n",
    "    \n",
    "    for k, res in top10.items(): \n",
    "        print(\"{} {:.2f}: {}\".format(res['rank'], scores[res['rank']], res['transcript']))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
