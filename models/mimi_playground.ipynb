{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mimig/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import rank_bm25 as bm25\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = '/media/sf_year2/DD2430-Project/Podcast-Segment-Retrieval-Spotify/'\n",
    "os.chdir(wd)\n",
    "files = os.listdir('./data/training_episodes')\n",
    "files = [f for f in files if f != 'place_holder']\n",
    "#files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    return text.translate(text.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    t = text.split(\" \")\n",
    "    res = [lemmatizer.lemmatize(w) for w in t]\n",
    "    \n",
    "    return \" \".join(res)\n",
    "\n",
    "def stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    t = text.split(\" \")\n",
    "    res = [stemmer.stem(w) for w in t]\n",
    "    \n",
    "    return \" \".join(res)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    t = re.sub(r'\\d+', \"\", text)\n",
    "\n",
    "    return t\n",
    "\n",
    "def clean_text(text):\n",
    "    t = text.lower()\n",
    "    t = strip_punctuation(t)\n",
    "    t = remove_stopwords(t)\n",
    "    t = remove_numbers(t)\n",
    "    # t = stem(t)\n",
    "    t = lemmatize(t)\n",
    "    return t\n",
    "    \n",
    "# Simon's  extraction function\n",
    "def extract_segments(path):\n",
    "    \"\"\"Given path to json file containing an episode extracts all segments of that episode, \n",
    "    including start and end time of each segment.\"\"\"\n",
    "    with open(path, \"r\") as read_file:\n",
    "        episode = json.load(read_file)\n",
    "    segments=[]\n",
    "    segment_transcripts = []\n",
    "    clean_segment_transcripts = []\n",
    "    #had to do \"manual\" iteration due to irregularities in data\n",
    "    iter=0\n",
    "    for segment in episode[\"results\"]:\n",
    "        seg_result={}\n",
    "        \n",
    "        #make sure there is only one dict in this list (should be true according to dataset description)\n",
    "        assert len(segment[\"alternatives\"])==1\n",
    "        segment_dict=segment[\"alternatives\"][0]\n",
    "        #sometimes \"alternatives\" dict is empty...\n",
    "        if \"words\" and \"transcript\"  in segment_dict:\n",
    "            #add segment number\n",
    "            seg_result[\"segNum\"]=iter\n",
    "            #add timestamp of the first word in this segment\n",
    "            seg_result[\"startTime\"]=segment_dict[\"words\"][0][\"startTime\"]\n",
    "            #add timestamp of the last word in this segment\n",
    "            seg_result[\"endTime\"]=segment_dict[\"words\"][-1][\"endTime\"]\n",
    "            #add transcript of this segment \n",
    "            tr = segment_dict[\"transcript\"]\n",
    "            seg_result[\"transcript_clean\"] = clean_text(tr)\n",
    "            seg_result[\"transcript\"]=tr\n",
    "            segment_transcripts.append(tr)\n",
    "            clean_segment_transcripts.append(seg_result[\"transcript_clean\"])\n",
    "            segments.append(seg_result)\n",
    "            iter+=1\n",
    "            \n",
    "    return {'segments': segments, \"full_text\": \"\".join(segment_transcripts), \"full_text_clean\": \"\".join(clean_segment_transcripts),}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_segments = []\n",
    "# all_starts = []\n",
    "# all_ends = []\n",
    "# corpus = []\n",
    "# for filename in files:\n",
    "#     with open(os.path.join('./data', filename), 'rb') as f:\n",
    "#         dat = json.load(f)\n",
    "\n",
    "#     segments = []\n",
    "#     starts = []\n",
    "#     ends = []\n",
    "#     for alt in dat['results']:\n",
    "#         if 'transcript' in alt['alternatives'][0].keys():\n",
    "#             seg = alt['alternatives'][0]['transcript']\n",
    "#             segments.append(seg)\n",
    "#             s = alt['alternatives'][0]['words'][0]['startTime']\n",
    "#             starts.append(s)\n",
    "#             e = alt['alternatives'][0]['words'][-1]['endTime']\n",
    "#             ends.append(e)\n",
    "\n",
    "#     text = \"\".join(segments)\n",
    "#     all_segments.append(segments)\n",
    "#     all_starts.append(starts)\n",
    "#     all_ends.append(ends)\n",
    "#     corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_sub = {}\n",
    "\n",
    "# for filename in files: \n",
    "#     training_sub[os.path.splitext(filename)[0]] = extract_segments(os.path.join(\"./data/training_episodes\", filename))\n",
    "\n",
    "# with open(\"./data/training_sub_cleaned.json\", \"w\") as f:\n",
    "#     json.dump(training_sub, f)\n",
    "\n",
    "with open(\"./data/training_sub_cleaned.json\", \"r\") as f:\n",
    "    training_sub = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = list(training_sub.keys())\n",
    "full_texts_clean = [training_sub[k]['full_text_clean']for k in list(training_sub.keys())]\n",
    "full_texts = [training_sub[k]['full_text']for k in list(training_sub.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, stop_words={'english'})\n",
    "X = vectorizer.fit_transform(full_texts_clean)\n",
    "tf_idf_df = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns = vectorizer.get_feature_names()\n",
    ")\n",
    "# tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"./data\", \"podcasts_2020_topics_train.xml\"), \"r\") as f:\n",
    "    topics = f.readlines()\n",
    "    \n",
    "topics = \"\".join(topics)  \n",
    "topics = xmltodict.parse(topics)\n",
    "\n",
    "temp = {}\n",
    "for t in topics['topics']['topic']:\n",
    "    n = int(t['num'])\n",
    "    temp[n] = {}\n",
    "    temp[n]['query'] = t['query']\n",
    "#     temp[n]['query_clean'] = clean_text(t['query'])\n",
    "    temp[n]['type'] = t['type']\n",
    "    temp[n]['description'] = t['description']\n",
    "    temp[n]['description_clean'] = clean_text(t['description'])\n",
    "    \n",
    "topics = temp\n",
    "# topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = topics[1][\"description_clean\"]\n",
    "tokens = [x for x in topic.split(\" \") if x not in [\"\", \" \"]]\n",
    "\n",
    "bigrams = []\n",
    "for gram in nltk.ngrams(tokens, 2):\n",
    "    bigrams.append(\" \".join(gram))\n",
    "\n",
    "    \n",
    "tokens.extend(bigrams)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.zeros((tf_idf_df.shape[0]))\n",
    "for t in tokens:\n",
    "    if t in tf_idf_df.columns:\n",
    "        score = score + tf_idf_df.loc[:, t]\n",
    "        \n",
    "top_episodes = np.argsort(-score)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people saying spread novel coronavirus ncov wuhan end \n",
      "5MM6w9RTRYj5ePligGvZNs\n",
      "You ready? If you're ready. Yes, you'll get a shout-out to Almighty supers out there. We are gathered here today to substitute eat, honey. So make sure you guys have your tea cups ready because you already know this T is what - Hi. How are you guys? Good afternoon. I hope everybody's doing good today. So I want to come on here with another podcast. So this is going to be very different from the typical videos that I usually do and this goes back to two days ago right before Kobe Bryant's death. So if you follow me on Instagram which unfortunately My page is private because of people trolling and being disrespectful. So it's bad private again before Kobe Bryant died. I was sent I was sent pictures of this new delicacy. That's all the rage right now in China, which is called bat soup, and it's been a delicacy for a while now and so me and one of my subscribers we're talking about this and she's like you got to post this. This is so disgusting, you know, I and we all know the Chinese they\n",
      "1sbXZEnTGyGKNfpEogahoP\n",
      "Hi, I'm Wendy Zuckerman and you're listening to science versus from gimlet. Today we're looking into what we know about this new coronavirus that showed up in China late last year and has since been found in at least 18 other countries in the u.s. Five people who recently visited China were diagnosed with this virus and on January 30th, we heard that the husband of one of those people is now also infected. It's the first case of human-to-human transmission in the US and while this is very much a moving Target since the outbreak Scientists have been scrambling to understand this virus. We went to dr. Catherine paulus. She's an infectious disease expert Katherine used to work at the National Institutes of Health and she's now at Penn State Health in Pennsylvania. If we started to have cases here at the hospital, I would be on the front lines of this and what's happening in your hospital right now right now, we're preparing for I think a very real possibility that we may see one of these \n",
      "1t6qb1pD2AfVVjyLn1Wisr\n",
      "Hey guys, this is 0 from The Conspiracy Files. And if you ever heard about anchor is the easiest way to make a podcast, let me explain first and foremost is free their creation tools that allow you to record and edit your podcast right from your phone or even your computer anchor will distribute your podcast for you. So it can be heard on Spotify Apple podcast and many more you can even make money from your podcast with theNo minimum listenership. It's everything you need to make a podcast in one place Go download the free anchor app or go to Anchor dot f m-- to get started. And now the other major headline is Friday night that deadly coronavirus soaring and numbers tonight a new case here in the US the CDC now confirming a second case a woman in Chicago. In fact 63 patients are now being tested here in the US for possible symptoms across 22 States and alarming new video tonight from China showing patience crowded into Wuhan Red Cross Hospital the staff and hazmat suits tonight, China.\n",
      "0E2nqCXMkS218SE72APmNr\n",
      "A quick word about my sponsor anchor if you've ever thought about creating your own podcast piece of advice for you. Use anchor anchor is your One-Stop shop for creating your own podcast. It's free. It allows you to record and edit your podcast right from your phone or your computer. It also distributes your podcast to most major podcast platforms. And finally, it gives you an opportunity to monetize your podcast. So if podcast or something you're looking to get into download the free anchor app or go to Anchor dot f mIt started. Hey, welcome back on this Friday this last Friday last day of this January month time flies by you know, as you can see from the title of today's podcast. We're talking to coronavirus again today. And I know I know it's you know, this has been basically what my channel has been coronavirus Central for like a week now and and then really, I think there's two facets to that for why that's the case. I mean first of all, I mean this is as far as markets as far as \n",
      "2tJJuf3cRuHpsCrEEZqshR\n",
      "A quick word about my sponsor anchor if you've ever thought about creating your own podcast piece of advice for you. Use anchor anchor is your One Stop Shop for creating your own podcast. It's free. It allows you to record and edit your podcast right from your phone or your computer. It also distributes your podcast to most major podcast platforms. And finally, it gives you an opportunity to monetize your podcast. So if podcast or something you're looking to get into download the free anchor app or go to Anchor dot f mIt started. Hey, welcome back. So I want to to make today's episode about this continuing spread of the Wuhan coronavirus. I think the official name for it is the 2019 novel coronavirus, but many people are referring to it as just the coronavirus the the Wuhan coronavirus Wuhan flew I've seen that go around but and I think a good place to start is just a quick recap. Where were at now of course is subject to change. And I'll be talking more about these numbers. But but of\n",
      "4zYUnfcd1a1jf0KbCWIcFs\n",
      "Hey, welcome back. So rather than throw an ad at the beginning of today's podcast. I wanted to quickly take you know, 30 seconds here to tell you about a pretty cool program called Delphia soon-to-be an app essentially the way Delphia works is that it is looking for data the same data that's already collected on us on a daily basis through Google through Facebook through GPS Etc and to pay you for that and in turn they're going to use that data to hopefully get a better idea of consuming.Gratitude since we do have a consumer-driven economy and to beat the average stock market return through that data. And in fact, you can even invest that money that they're giving you for your data into to the market with with their model now, I don't know how well their model is going to work time will tell but there's not a whole lot to lose except for your data that's already being given out pretty freely or being taken by a lot of these companies. So a little over 30 seconds here, but but anyways, \n",
      "6NFE7Hc6BSKxyYwLP0tU7e\n",
      "hot Hi and welcome to the next episode of the germs and worms podcast. My name is Miguel and my co-host Anna will be joining us next time so it's just me this time. I just wanted to get this podcast out as soon as possible because this week we're talking about the Wuhan virus. Now. I know it's easy to confuse it. It's not the Wu-Tang virus. It's the blue hand virus. So this outbreak of this new Wuhan virus is all over the news, but sometimes you only get a few seconds of information about it. So what I'm hoping to do is you'll know enough about the virus and the outbreak You're listening to this podcast that you'll be able to while all your friends is weekend. Okay, let's start with the basics. So why is it called the Wuhan virus? Well, of course it came from the Wuhan city and region in China and it's not a little city. It's a city of 11 million people and in the city, there's a very famous produce and meat market. This is a called a wet Market where where there's a lot of live and de\n",
      "7w98tfnOOcXOFtU5It9Mfz\n",
      "Hi, I'm Wendy Zuckerman and you're listening to science. This is from gimlet. We are on a break. We were on a break working on new episodes round next season that was until we started hearing about this new virus infecting people in China with the number of cases going up and up. There are nearly 300 confirmed cases 830 confirmed cases. They were heading towards 3,000 more than 4,500 have been infected China has gone into full blown crisis mode building new hospitals closing schools and shutting Transport between cities effectively putting millions of people under lockdown the country has quarantined multiple cities with streets are deserted shops are shut it is cut off from the rest of the world. But still there are cases showing up around the world. He says confirmed in Japan South Korea and Thailand and Europe Australia to the United States right now, though. The vast majority of infected people are in China as of January 30th. That's today Chinese. Officials have reported almost 10\n",
      "6qbgrdPtRuEAvaOUfOdYMY\n",
      "Well, if you can't tell I'm congested. I'm sick. Yes, and I'm worried about you friend because I can't even laugh. Did you hear that? But it's not just me. I feel like everybody is sick. Yes, and I understand it. But I also don't understand it. Right and the thing that is making us all feel very nervous about everybody. That's sick is the coronavirus. Yeah. There's pandemonium Panic about coronavirus, you know. On my Instagram and Twitter y'all are out here wearing nail shop mask and it's not right. I haven't said anything. I saw some of Walker had on gloves and masks. She's not playing around. I've seen a lot of people that have been using all types of tricks to try and keep the germs away my poor friend. You hear that sad laughs we're gonna get through this episode. We will I'm gonna try to suppress the cough. I'm TT and I'm Zakia and from Spotify Studios. This is dope labs. This episode is all about coronavirus and we're going to go through everything we know up until this point wit\n",
      "1ZA1QTtylexrVt75xiprNH\n",
      "Hello, this is dr. Dan Guerra. Come to you pathetic biochemistry Studios. And the Inland Pacific Northwest today is the 30th of January 2020. I'm going to break away a little bit from my our discussion on autoimmune diseases because of the Corona virus outbreak promise to my audience and I would make a discussion of it from the authentic biochemistry Studios and from the perspective of the depth that we normally do which means going into the primary literature examining the virus itself looking at its molecular organization mode of transmission is pathogenicity and also some of the treatments that Put out there. I'm also going to give you a small history of the Meuse and SARS coronavirus which were precursors to the current one before we get started on that. I just want to make a little plug for our podcast. I'm in the middle of a campaign to try to get funding from the Grassroots level and you can go to my Facebook site and my Facebook is Of course, www.facebook.com / Dan Guerra 0 0 b\n"
     ]
    }
   ],
   "source": [
    "print(topic)\n",
    "for i in top_episodes:\n",
    "    print(episodes[i])\n",
    "    print(full_texts[i][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "# flat_segments = flatten(all_segments)\n",
    "# tokenized_segments = [x.split(\" \") for x in flat_segments]\n",
    "# tokenized_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BM25Okapi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fdb97be1174c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbm25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25Okapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_segments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"climate change strike\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenized_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BM25Okapi' is not defined"
     ]
    }
   ],
   "source": [
    "bm25 = BM25Okapi(tokenized_segments)\n",
    "\n",
    "query = \"climate change strike\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "bm25.get_top_n(tokenized_query, flat_segments, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
