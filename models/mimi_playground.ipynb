{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6VKk8uVLTJV4BUARHyf1uG.json', '7sHyO8wLeEd1LuxfS8AIls.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = '/media/sf_year2/DD2430-Project/Podcast-Segment-Retrieval-Spotify/'\n",
    "os.chdir(wd)\n",
    "files = os.listdir('./data')\n",
    "files = [f for f in files if f != 'place_holder']\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = files[0]\n",
    "with open(os.path.join('./data', f1), 'rb') as f:\n",
    "    f1_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments = []\n",
    "all_starts = []\n",
    "all_ends = []\n",
    "corpus = []\n",
    "for filename in files:\n",
    "    with open(os.path.join('./data', filename), 'rb') as f:\n",
    "        dat = json.load(f)\n",
    "\n",
    "    segments = []\n",
    "    starts = []\n",
    "    ends = []\n",
    "    for alt in dat['results']:\n",
    "        if 'transcript' in alt['alternatives'][0].keys():\n",
    "            seg = alt['alternatives'][0]['transcript']\n",
    "            segments.append(seg)\n",
    "            s = alt['alternatives'][0]['words'][0]['startTime']\n",
    "            starts.append(s)\n",
    "            e = alt['alternatives'][0]['words'][-1]['endTime']\n",
    "            ends.append(e)\n",
    "\n",
    "    text = \"\".join(segments)\n",
    "    all_segments.append(segments)\n",
    "    all_starts.append(starts)\n",
    "    all_ends.append(ends)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christmas\n",
      "0.0\n",
      "0.012982360840821777\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, stop_words={'english'})\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "w = 1998\n",
    "print(vectorizer.get_feature_names()[w])\n",
    "print(X[0, w])\n",
    "print(X[1, w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "flat_segments = flatten(all_segments)\n",
    "tokenized_segments = [x.split(\" \") for x in flat_segments]\n",
    "# tokenized_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" I don't like that strike the fuck up. What is Donkey Kong, you know bang, you know, when you hit them off of over the head, I'll give you got a Donkey Kong. Laughs well, oh my Heavens, I'm almost gonna beat the Rams your will, you know, when they're you had to lose feeling in her eye. That was almost me. Uh, see, you know, I feel like if you go down hey,\",\n",
       " \" Well rounded Queen shout-out and well Rodney King's as well. Yes, baby. I'm telling you call switching like a mug sometimes better than what's up. What you do got to work voice not because I don't got it on the phone like that and you go but when I did I get on the phone you had to work boys. I changed my posture and if you change your posture, yeah because you got to get some air right in here.\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25 = BM25Okapi(tokenized_segments)\n",
    "\n",
    "query = \"climate change strike\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "bm25.get_top_n(tokenized_query, flat_segments, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
